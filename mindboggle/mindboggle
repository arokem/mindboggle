#!/usr/bin/env python
"""
This is the main program to run Mindboggle.

For help in using Mindboggle ::

    - Online `documentation <http://mindboggle.info/documentation.html>`_
    - README file
    - Help on the command line::

        $ mindboggle --help

This file uses Nipype (http://www.nipy.org/nipype/) to create a workflow
environment that enables Mindboggle to run in a flexible, modular manner
while storing provenance information.

Authors:
    - Arno Klein, 2010-2015  (arno@mindboggle.info)  http://binarybottle.com
    - Satrajit S. Ghosh, 2013  (satra@mit.edu)  http://www.mit.edu/~satra/
    - Each file lists Mindboggle team members who contributed to its content.

Copyright 2015,  Mindboggle team (http://mindboggle.info), Apache v2.0 License

"""

import os
import argparse
# ----------------------------------------------------------------------------
# Nipype libraries
# ----------------------------------------------------------------------------
from nipype import config, logging
from nipype.interfaces.ants import ApplyTransforms
from nipype.interfaces.io import DataGrabber, DataSink
from nipype.interfaces.utility import Function as Fn
from nipype.interfaces.utility import IdentityInterface
from nipype.pipeline.engine import Workflow, Node
# ----------------------------------------------------------------------------
# Mindboggle libraries
# ----------------------------------------------------------------------------
from mindboggle.features.folds import extract_folds
from mindboggle.features.fundi import extract_fundi, segment_fundi
from mindboggle.features.sulci import extract_sulci
from mindboggle.guts.mesh import rescale_by_neighborhood
from mindboggle.guts.paths import smooth_skeleton
from mindboggle.guts.relabel import relabel_surface, relabel_volume, \
    keep_volume_labels, remove_volume_labels, overwrite_volume_labels
from mindboggle.guts.segment import combine_2labels_in_2volumes
from mindboggle.guts.utilities import list_strings
from mindboggle.mio.convert_volumes import convert2nii
from mindboggle.mio.fetch_data import hashes_url, fetch_check_data, \
    fetch_ants_data
from mindboggle.mio.labels import DKTprotocol
from mindboggle.mio.tables import write_shape_stats, write_vertex_measures
from mindboggle.mio.vtks import read_vtk, apply_affine_transforms, \
    freesurfer_surface_to_vtk, freesurfer_curvature_to_vtk, \
    freesurfer_annot_to_vtk
from mindboggle.shapes.laplace_beltrami import spectrum_per_label
from mindboggle.shapes.likelihood import compute_likelihood
from mindboggle.shapes.surface_shapes import area, curvature, travel_depth, \
    geodesic_depth
from mindboggle.shapes.volume_shapes import thickinthehead, volume_per_brain_region
from mindboggle.shapes.zernike.zernike import zernike_moments_per_label
from mindboggle.thirdparty.ants import PropagateLabelsThroughMask

# ============================================================================
#
#   Command-line arguments
#
# ============================================================================
parser = argparse.ArgumentParser(description="""
                    The Mindboggle software automates shape analysis of
                    anatomical labels and features extracted from human brain
                    MR image data. For more information, please see
                    http://mindboggle.info/users/README.html""",
                     formatter_class = lambda prog:
                     argparse.HelpFormatter(prog, max_help_position=40))

features_group = parser.add_argument_group('more features (or --all)')
shapes_group = parser.add_argument_group('more shapes (or --all)')
inputs_group = parser.add_argument_group('modify inputs')
outputs_group = parser.add_argument_group('modify outputs')

# "positional arguments":
parser.add_argument("SUBJECT",
                    help=('the name of a subdirectory within the subjects'
                          ' directory (see --subjects_dir) containing subject'
                          ' surface and/or volume data,'
                          ' usually generated by the FreeSurfer software'))
# "optional arguments":
parser.add_argument("--all", help="compute all shape measures on all features"
                                  " (except where noted)",
                    action='store_true')
parser.add_argument("--ants",
                    help=("antsCorticalThickness.sh segmented file (ex:"
                          " ~/ants_data/bert/antsBrainSegmentation.nii.gz);"
                          " if --ants is set, then transforms are also"
                          " accessed from the same directory"),
                    metavar='STR')
parser.add_argument("--subjects_dir",
                    help=("directory containing SUBJECT subdirectory, if not"
                          " equal to environment variable $SUBJECTS_DIR"),
                    metavar='STR')
parser.add_argument("--plugin", dest="plugin",
                    default='Linear',
                    help="Plugin to use, such as: --plugin PBS")
parser.add_argument("--plugin_args", dest="plugin_args",
                    help="Plugin arguments in dictionary form, such as:"
                         " --plugin_args \"dict(qsub_args='-q many')\"")
parser.add_argument("-p", "--proc",
                    help=('number of processors (default: 1)'),
                    type=int, default=1, metavar='INT')
parser.add_argument("-g", "--graph",
                    help=('plot workflow graph: "hier", "flat", "exec"'),
                    choices=['hier', 'flat', 'exec'], metavar='STR')
parser.add_argument("-v", "--version", help="mindboggle version number",
                    action='version', version='%(prog)s 0.1')
features_group.add_argument("--folds", action='store_true',
                    help="save all (unidentified) folds")
features_group.add_argument("--sulci", action='store_true',
                    help="extract, identify, and measure sulci in folds")
features_group.add_argument("--fundi", action='store_true',
                    help="extract, identify, and measure fundi in sulci"
                         " [UNTESTED, not included in --all]")
features_group.add_argument("--points", action='store_true',
                    help=("table of per-vertex surface shape measures"))
features_group.add_argument("--surfaces_in_mni", action='store_true',
                    help="cortical surfaces in MNI152 space")
shapes_group.add_argument("--thickness", action='store_true',
                    help="cortical label thicknesses computed from volume,"
                         " not surface data (volume divided by middle area)")
shapes_group.add_argument("--moments",
                    help="Zernike moments order per label/feature"
                         " (ex: 10) [not included in --all"
                         " due to memory leak]",
                    default=0, type=int, metavar='INT')
shapes_group.add_argument("--spectra",
                    help="Laplace-Beltrami spectrum eigenvalues"
                         " per label/feature (ex: 10)",
                    default=0, type=int, metavar='INT')
inputs_group.add_argument("--my_atlas",
                    help=("alternate atlas to OASIS-TRT-20 jointfusion atlas;"
                          " must follow DKT31 cortical and CMA noncortical"
                          " labeling protocols, be in MNI152 space,"
                          " and should correspond to Atropos template used"
                          " in antsCorticalThickness.sh"),
                    metavar='STR')
inputs_group.add_argument("--my_atlases",
                    help=("extra labeling volume atlas(es) in MNI152 space;"
                          " use any label numbers (disregard names) in"
                          " LUT.py's return_numbers_names_colors() function"),
                    nargs='+', metavar='')
inputs_group.add_argument("--my_segments",
                          help=("use this non/cortex-segmented file,"
                                " for example when running Mindboggle on"
                                " edited version of FreeSurfer-ANTs hybrid"
                                " segmentation generated by Mindboggle"
                                " (must still call --ants for transforms)"
                                " "),
                    metavar='STR')
inputs_group.add_argument("--volume_labels",
                    help=(argparse.SUPPRESS),
                    choices=['wmparc', 'aparc+aseg'],
                    default='wmparc', metavar='STR')
inputs_group.add_argument("--surface_labels",
                    help=(argparse.SUPPRESS),
                    choices=['freesurfer', 'manual'],
                    default='freesurfer', metavar='STR')
inputs_group.add_argument("--my_transform",
                    help=("alternate affine transform from Atropos template"
                          " to MNI152 space, in case the OASIS-30 Atropos"
                          " template was not used in antsCorticalThickness.sh"
                          " (format should match antsAffine.sh ITK transform)"),
                    metavar='STR')
outputs_group.add_argument("--out",
                    help='output folder (if not $HOME/mindboggled)',
                    default=os.path.join(os.environ['HOME'],
                                         'mindboggled'), metavar='STR')
outputs_group.add_argument("--working",
                    help="working folder"
                         " (if not $HOME/mindboggle_working)",
                    default=os.path.join(os.environ['HOME'],
                                         'mindboggle_working'), metavar='STR')
outputs_group.add_argument("--cache", help='download folder '
                                    '(if not $HOME/mindboggle_cache)',
                    default=os.path.join(os.environ['HOME'],
                                         'mindboggle_cache'), metavar='STR')
outputs_group.add_argument("--no_volumes", action='store_true',
                    help="no volume labels, features, or shape tables")
outputs_group.add_argument("--no_surfaces", action='store_true',
                    help="no surface labels, features, or shape tables")
outputs_group.add_argument("--no_labels", action='store_true',
                    help="no surface or volume labels")
outputs_group.add_argument("--no_shapes", action='store_true',
                    help="no shape tables of surface labels or features")
outputs_group.add_argument("--save_only_tables", action='store_true',
                    help="save only tables to free up space")
args = parser.parse_args()

# ----------------------------------------------------------------------------
# Data arguments:
# ----------------------------------------------------------------------------
subject = args.SUBJECT
subjects_dir = args.subjects_dir
if not subjects_dir:
    if os.environ['SUBJECTS_DIR']:
        subjects_dir = os.environ['SUBJECTS_DIR']
    else:
        raise(IOError('Please set "subjects_dir" or $SUBJECTS_DIR variable.'))
if args.my_atlas:
    my_atlas = args.my_atlas
else:
    my_atlas = None
if args.my_segments:
    my_segments = args.my_segments
else:
    my_segments = None
if args.my_transform:
    my_transform = args.my_transform
else:
    my_transform = None
use_FS_inputs = True
do_input_vtk = False  # Load VTK surfaces directly (not FreeSurfer surfaces)
do_input_fs_labels = False  # Load nifti (not FreeSurfer mgh file)

# ----------------------------------------------------------------------------
# Label, feature, and shape arguments:
# ----------------------------------------------------------------------------
volume_labels = args.volume_labels
surface_labels = args.surface_labels
if args.ants:
    ants_seg = args.ants
    use_ants = True
else:
    use_ants = False
if args.no_labels:
    do_label = False
else:
    do_label = True
if args.no_shapes:
    do_shapes = False
else:
    do_shapes = True
if args.save_only_tables:
    save_all = False
else:
    save_all = True

# FreeSurfer shapes:
if do_shapes and use_FS_inputs:
    do_freesurfer_thickness = True
    do_freesurfer_curvature = True
    do_freesurfer_sulc = True
else:
    do_freesurfer_thickness = False  # Include FreeSurfer's thickness measure
    do_freesurfer_curvature = False  # Include FreeSurfer's curvature (curv)
    do_freesurfer_sulc = False  # Include FreeSurfer's convexity (sulc)

# Settings resulting from the --all argument:
all_features_shapes = args.all
do_fundi = args.fundi
spectra = args.spectra
moments = args.moments
if moments > 0:
    do_moments = True
else:
    do_moments = False
if all_features_shapes:
    save_folds = True
    do_sulci = True
#    do_fundi = True
    do_points = True
    do_surfaces_in_mni = True
    do_thickinthehead = True
    # Set defaults for spectra and moments if not supplied and --all is set
    if spectra <= 0:
        spectra = 10
    do_spectra = True
#    if moments <= 0:
#        moments = 10
#    do_moments = True
else:
    save_folds = args.folds
    do_sulci = args.sulci
#    do_fundi = args.fundi
    do_points = args.points
    do_surfaces_in_mni = args.surfaces_in_mni
    do_thickinthehead = args.thickness
    do_spectra = False
#    do_moments = False

# Extra labeling atlases in MNI space:
atlases = args.my_atlases
add_atlas_names = []
if atlases:
    if isinstance(atlases, str):
        atlases = [atlases]
    for add_atlas in atlases:
        add_atlas_names.append(os.path.basename(add_atlas).split('.')[0])

# ============================================================================
#
#   Hidden arguments: paths, labels and template data
#
# ============================================================================
overwrite_cerebrum_with_cerebellum = True
fill_noncortex_with_ants_labels = False
do_smooth_fundi = False
# ----------------------------------------------------------------------------
# Path to C++ code:
# ----------------------------------------------------------------------------
ccode_path = os.environ['MINDBOGGLE_TOOLS']  # Mindboggle C++ code directory
# ----------------------------------------------------------------------------
# Hashes to verify retrieved data, and output, working, and cache directories:
# ----------------------------------------------------------------------------
hashes, url, cache_env, cache = hashes_url()
if args.cache:
    cache = args.cache
elif cache_env in os.environ.keys():
    cache = os.environ[cache_env]
if args.working:
    working = os.path.join(args.working, subject)
else:
    working = os.path.join(os.environ['HOME'], 'mindboggle_working', subject)
if not os.path.isdir(args.out):
    print("Create missing output directory: {0}".format(args.out))
    os.makedirs(args.out)
if not os.path.isdir(working):
    print("Create missing working directory: {0}".format(working))
    os.makedirs(working)
if not os.path.isdir(cache):
    print("Create missing cache directory: {0}".format(cache))
    os.makedirs(cache)
# ----------------------------------------------------------------------------
# Labeling protocol information and volume atlases:
# ----------------------------------------------------------------------------
dkt = DKTprotocol()
atlas_volume = 'OASIS-TRT-20_jointfusion_DKT31_CMA_labels_in_MNI152_v2.nii.gz'
atropos_to_MNI152_affine = 'OASIS-30_Atropos_template_to_MNI152_affine.txt'
# ----------------------------------------------------------------------------
# Surface atlas labels:
# - 'manual': manual edits
# - FUTURE: <'adjusted': manual edits after automated alignment to fundi>
# ----------------------------------------------------------------------------
surface_atlas_type = 'manual'
#modify_surface_labels = False

# ============================================================================
#
#   Basic functions
#
# ============================================================================
def split_list_pair(List):
    element1 = List[0]
    element2 = List[1]
    return element1, element2

def first_string_containing_substring(substring, List):
    first_matching_string = [x for x in List if substring in x][0]
    return first_matching_string

# ============================================================================
#
#   Initialize workflow inputs and outputs
#
# ============================================================================
mbFlow = Workflow(name='Mindboggle')
mbFlow.base_dir = working

# ----------------------------------------------------------------------------
# Iterate inputs over hemispheres and atlases
# (surfaces are assumed to take the form: lh.pial or lh.pial.vtk)
# ----------------------------------------------------------------------------
if add_atlas_names:
    InputAddAtlases = Node(name='Input_volume_atlases',
                           interface=IdentityInterface(fields=['atlas']))
    InputAddAtlases.iterables = ('atlas', add_atlas_names)
InputHemis = Node(name='Input_hemispheres',
                  interface=IdentityInterface(fields=['hemi']))
hemis = ['lh', 'rh']
InputHemis.iterables = ('hemi', hemis)
# ----------------------------------------------------------------------------
# Outputs and name substitutions
# ----------------------------------------------------------------------------
Sink = Node(DataSink(), name='Results')
Sink.inputs.base_directory = args.out
Sink.inputs.container = subject

if my_segments:
    seg_in = os.path.basename(my_segments)
else:
    seg_in = 'combined_segmentations.nii.gz'
mgz = volume_labels + '.mgz.nii.gz'
ants_str = 'ants_labels.nii.gz'
fs_filled_fs = '{0}_to_{0}_through_{0}_to_{0}_through_{0}'.format(mgz)
fs_filled = '{0}_to_{0}_through_{1}_to_{0}_through_{1}'.format(mgz, seg_in)
ants_filled_ants = '{0}_to_{1}_to_{1}_through_{0}'.format(mgz, ants_str)


ants_filled = '{0}_to_{1}_to_{1}_through_{2}'.format(mgz, ants_str, seg_in)


fs_filled_fs_rename = 'freesurfer_' + volume_labels + \
                      '_filled_labels_in_freesurfer_segmentation.nii.gz'
fs_filled_rename = 'freesurfer_' + volume_labels + '_filled_labels.nii.gz'
ants_filled_ants_rename = 'ants_filled_labels_in_ants_segmentation.nii.gz'
ants_filled_rename = 'ants_filled_labels.nii.gz'

Sink.inputs.substitutions = [ ('lh.', ''), ('rh.', ''),
    ('_hemi_lh', 'left_cortical_surface'),
    ('_hemi_rh', 'right_cortical_surface'),
    ('pial.', ''),
    ('thickness.vtk', 'freesurfer_thickness.vtk'),
    ('curv.vtk', 'freesurfer_curvature.vtk'),
    ('sulc.vtk', 'freesurfer_sulc.vtk'),
    ('relabeled_aparc.vtk', 'freesurfer_cortex_labels.vtk'),
    (fs_filled_fs, fs_filled_fs_rename),
    (fs_filled, fs_filled_rename),
    (ants_filled_ants, ants_filled_ants_rename),
    (ants_filled, ants_filled_rename),
    ('smooth_skeletons.vtk', 'smooth_fundi.vtk')]
# Substitutions for additional atlas names:
Sink.inputs.regexp_substitutions = [
    (r'/_atlas_(.*)/ants_added_atlas_labels.nii.gz', r'/\1_labels.nii.gz'),
    (r'/_atlas_(.*)/volume_for_each_added_label.csv',
     r'/volume_for_each_\1_label.csv'),
    (r'/affine_(.*).vtk', r'/cortex_in_MNI152_space.vtk')]

# ----------------------------------------------------------------------------
# Affine transform from Atropos template to MNI152 space:
# ----------------------------------------------------------------------------
if my_transform:
    affine_template2mni = my_transform
else:
    affine_template2mni = fetch_check_data(atropos_to_MNI152_affine,
                                           url, hashes, cache_env, cache)
# ----------------------------------------------------------------------------
# Atlas labels:
# ----------------------------------------------------------------------------
if do_label and not my_atlas:
    FetchAtlas = Node(name='Fetch_atlas',
                      interface=Fn(function=fetch_check_data,
                                   input_names=['data_file',
                                                'url',
                                                'hashes',
                                                'cache_env',
                                                'cache',
                                                'return_missing',
                                                'lookup'],
                                   output_names=['data_path']))
    mbFlow.add_nodes([FetchAtlas])
    FetchAtlas.inputs.data_file = atlas_volume
    FetchAtlas.inputs.url = url
    FetchAtlas.inputs.hashes = hashes
    FetchAtlas.inputs.cache_env = cache_env
    FetchAtlas.inputs.cache = cache
    FetchAtlas.inputs.return_missing = False
    FetchAtlas.inputs.lookup = True

# ----------------------------------------------------------------------------
# ANTs data:
# ----------------------------------------------------------------------------
if use_ants:
    FetchANTs = Node(name='Fetch_ants_data',
                     interface=Fn(function=fetch_ants_data,
                                  input_names=['segmented_file',
                                               'use_ants_transforms'],
                                  output_names=['mask',
                                                'segments',
                                                'affine_subject2template',
                                                'warp_subject2template',
                                                'affine_template2subject',
                                                'warp_template2subject']))
    mbFlow.add_nodes([FetchANTs])
    FetchANTs.inputs.segmented_file = ants_seg
    FetchANTs.inputs.use_ants_transforms = True
    # ------------------------------------------------------------------------
    # For transforming volume labels --
    # Make list of ANTs MNI152-to-subject nonlinear transforms
    # to use antsApplyTransforms:
    #
    # Note regarding antsApplyTransforms:
    # To warp the subject image to the template, one would call
    # antsApplyTransforms...-i ${subject} -r ${template}
    #                       -t ${prefix}SubjectToTemplate1Warp.nii.gz
    #                       -t ${prefix}SubjectToTemplate0GenericAffine.mat
    # To warp the template image to the subject, one would call
    # antsApplyTransforms...-i ${template} -r ${subject}
    #                       -t ${prefix}TemplateToSubject1GenericAffine.mat
    #                       -t ${prefix}TemplateToSubject0Warp.nii.gz
    # ------------------------------------------------------------------------
    WarpToSubjectFileList = Node(name='Merge_warp_file_list',
                                 interface=Fn(function=list_strings,
                                      input_names=['string1',
                                                   'string2',
                                                   'string3',
                                                   'string4'],
                                      output_names=['string_list']))
    mbFlow.connect(FetchANTs, 'affine_template2subject',
                   WarpToSubjectFileList, 'string1')
    mbFlow.connect(FetchANTs, 'warp_template2subject',
                   WarpToSubjectFileList, 'string2')
    WarpToSubjectFileList.inputs.string3 = affine_template2mni
    WarpToSubjectFileList.inputs.string4 = ''
    warp_inverse_Booleans = [False, False, True]  # Boolean list
    # ------------------------------------------------------------------------
    # For transforming surface points --
    # Make list of subject-to-MNI affine transforms
    # to use antsApplyTransformsToPoints:
    #
    # Note regarding antsApplyTransformsToPoints:
    # Points are transformed in the OPPOSITE direction of images,
    # so you pass the inverse of what is needed to warp the images.
    # Note 2: Don't forget to switch the order of the affine transforms!
    # ------------------------------------------------------------------------
    AffineFileList = Node(name='Affine_file_list',
                          interface=Fn(function=list_strings,
                                       input_names=['string1',
                                                    'string2',
                                                    'string3',
                                                    'string4'],
                                       output_names=['string_list']))
    mbFlow.connect(FetchANTs, 'affine_subject2template',
                   AffineFileList, 'string1')
    AffineFileList.inputs.string2 = affine_template2mni
    AffineFileList.inputs.string3 = ''
    AffineFileList.inputs.string4 = ''
    inverse_Booleans = [1, 1]  # list of ones/zeros for True/False

# ============================================================================
# ----------------------------------------------------------------------------
#
#   Surface workflows
#
# ----------------------------------------------------------------------------
# ============================================================================
if not args.no_surfaces:
    # ------------------------------------------------------------------------
    # Location and structure of the surface inputs:
    # ------------------------------------------------------------------------
    use_white_surface = False
    if use_white_surface:
        Surf = Node(name='Surfaces',
                    interface=DataGrabber(infields=['subject', 'hemi'],
                                          outfields=['surface_files',
                                                     'white_surface_files'],
                                          sort_filelist=False))
    else:
        Surf = Node(name='Surfaces',
                    interface=DataGrabber(infields=['subject', 'hemi'],
                                          outfields=['surface_files'],
                                          sort_filelist=False))
    Surf.inputs.base_directory = subjects_dir
    Surf.inputs.template = '%s/surf/%s.%s'
    Surf.inputs.template_args['surface_files'] = [['subject', 'hemi', 'pial']]
    if use_white_surface:
        Surf.inputs.template_args['white_surface_files'] = [['subject',
                                                             'hemi', 'white']]
    #Surf.inputs.template_args['sphere_files'] = [['subject','hemi','sphere']]
    if do_freesurfer_thickness:
        Surf.inputs.template_args['freesurfer_thickness_files'] = \
            [['subject', 'hemi', 'thickness']]
    if do_freesurfer_curvature:
        Surf.inputs.template_args['freesurfer_curvature_files'] = \
            [['subject', 'hemi', 'curv']]
    if do_freesurfer_sulc:
        Surf.inputs.template_args['freesurfer_sulc_files'] = \
            [['subject', 'hemi', 'sulc']]

    Surf.inputs.subject = subject
    mbFlow.connect(InputHemis, 'hemi', Surf, 'hemi')
    # ------------------------------------------------------------------------
    # Convert surfaces to VTK:
    # ------------------------------------------------------------------------
    if not do_input_vtk:
        ConvertSurf = Node(name='Surface_to_vtk',
                           interface=Fn(function=freesurfer_surface_to_vtk,
                                        input_names=['surface_file',
                                                     'output_vtk'],
                                        output_names=['output_vtk']))
        mbFlow.connect(Surf, 'surface_files', ConvertSurf, 'surface_file')
        ConvertSurf.inputs.output_vtk = ''
        if use_white_surface:
            ConvertWhiteSurf = ConvertSurf.clone('Gray-white_surface_to_vtk')
            mbFlow.add_nodes([ConvertWhiteSurf])
            mbFlow.connect(Surf, 'white_surface_files',
                           ConvertWhiteSurf, 'surface_file')
    # ------------------------------------------------------------------------
    # Evaluation inputs: location and structure of atlas surfaces:
    # ------------------------------------------------------------------------
    if surface_labels == 'manual' and do_label:
        SurfaceAtlas = Node(name='Surface_atlas',
                            interface=DataGrabber(infields=['subject','hemi'],
                                                  outfields=['atlas_file'],
                                                  sort_filelist=False))
        SurfaceAtlas.inputs.base_directory = subjects_dir
        SurfaceAtlas.inputs.template = '%s/label/%s.labels.DKT31.' +\
                                       surface_atlas_type + '.vtk'
        SurfaceAtlas.inputs.template_args['atlas_file'] = [['subject','hemi']]

        SurfaceAtlas.inputs.subject = subject
        mbFlow.connect(InputHemis, 'hemi', SurfaceAtlas, 'hemi')

    # ========================================================================
    #
    #   Surface labels
    #
    # ========================================================================
    if do_label:
        SurfLabelFlow = Workflow(name='Surface_labels')

        # ====================================================================
        # Initialize labels with the DKT classifier atlas
        # NOTE: This can now be done by running FreeSurfer's recon-all
        #       with "-gcs DKTatlas40.gcs"
        # ====================================================================
        # surface_classifier = DKTatlas40
        # if surface_labels == 'atlas' and use_FS_inputs:
        #     # --------------------------------------------------------------
        #     # Label brain with DKT atlas using FreeSurfer's mris_ca_label:
        #     # --------------------------------------------------------------
        #     Classifier = Node(name='mris_ca_label',
        #                       interface=Fn(function=label_with_classifier,
        #                                    input_names=['subject',
        #                                                 'hemi',
        #                                                 'left_classifier',
        #                                                 'right_classifier',
        #                                                 'annot_file',
        #                                                 'subjects_directory'],
        #                                    output_names=['annot_file']))
        #     SurfLabelFlow.add_nodes([Classifier])
        #     Classifier.inputs.subject = subject
        #     mbFlow.connect(InputHemis, 'hemi',
        #                    SurfLabelFlow, 'mris_ca_label.hemi')
        #     left_classifier_file = 'lh.' + surface_classifier + '.gcs'
        #     right_classifier_file = 'rh.' + surface_classifier + '.gcs'
        #     left_classifier = fetch_check_data(left_classifier_file, url,
        #                                     hashes, cache_env, cache)
        #     right_classifier = fetch_check_data(right_classifier_file, url,
        #                                      hashes, cache_env, cache)
        #     Classifier.inputs.left_classifier = left_classifier
        #     Classifier.inputs.right_classifier = right_classifier
        #     Classifier.inputs.annot_file = ''
        #     Classifier.inputs.subjects_directory = subjects_dir
        #     # --------------------------------------------------------------
        #     # Convert .annot file to VTK format:
        #     # --------------------------------------------------------------
        #     Classifier2vtk = Node(name='annot_to_vtk',
        #                           interface=Fn(function=freesurfer_annot_to_vtk,
        #                                        input_names=['annot_file',
        #                                                     'vtk_file'],
        #                                        output_names=['labels',
        #                                                      'output_vtk']))
        #     SurfLabelFlow.add_nodes([Classifier2vtk])
        #     SurfLabelFlow.connect(Classifier, 'annot_file',
        #                           Classifier2vtk, 'annot_file')
        #     if do_input_vtk:
        #         mbFlow.connect(Surf, 'surface_files',
        #                        SurfLabelFlow, 'annot_to_vtk.vtk_file')
        #     else:
        #         mbFlow.connect(ConvertSurf, 'output_vtk',
        #                        SurfLabelFlow, 'annot_to_vtk.vtk_file')
        #     #if save_all:
        #     #    mbFlow.connect(SurfLabelFlow, 'annot_to_vtk.output_vtk',
        #     #                   Sink, 'labels.@DKT_surface')
        #     plug = 'annot_to_vtk.output_vtk'
        #     plug1 = Classifier2vtk
        #     plug2 = 'output_vtk'

        # ====================================================================
        # Initialize labels with FreeSurfer
        # ====================================================================
        if surface_labels == 'freesurfer' and use_FS_inputs:
            # ----------------------------------------------------------------
            # Location and structure of the FreeSurfer label inputs:
            # ----------------------------------------------------------------
            if surface_labels == 'freesurfer':
                Annot = Node(name='annot',
                             interface=DataGrabber(infields=['subject',
                                                             'hemi'],
                                                   outfields=['annot_files'],
                                                   sort_filelist=False))
                Annot.inputs.base_directory = subjects_dir
                Annot.inputs.template = '%s/label/%s.aparc.annot'
                Annot.inputs.template_args['annot_files'] = [['subject',
                                                              'hemi']]
                Annot.inputs.subject = subject
                mbFlow.connect(InputHemis, 'hemi', Annot, 'hemi')
            # ----------------------------------------------------------------
            # Convert Annot to VTK format:
            # ----------------------------------------------------------------
            FreeLabels = Node(name='FreeSurfer_annot_to_vtk',
                              interface=Fn(function=freesurfer_annot_to_vtk,
                                           input_names=['annot_file',
                                                        'vtk_file'],
                                           output_names=['labels',
                                                         'output_vtk']))
            SurfLabelFlow.add_nodes([FreeLabels])
            mbFlow.connect(Annot, 'annot_files', SurfLabelFlow,
                           'FreeSurfer_annot_to_vtk.annot_file')
            if do_input_vtk:
                mbFlow.connect(Surf, 'surface_files', SurfLabelFlow,
                               'FreeSurfer_annot_to_vtk.vtk_file')
            else:
                mbFlow.connect(ConvertSurf, 'output_vtk', SurfLabelFlow,
                               'FreeSurfer_annot_to_vtk.vtk_file')
            plug = 'FreeSurfer_annot_to_vtk.output_vtk'
            plug1 = FreeLabels
            plug2 = 'output_vtk'

        # ====================================================================
        # Skip label initialization and process manual (atlas) labels
        # ====================================================================
        elif surface_labels == 'manual':
            ManualSurfLabels = Node(name='Manual_surface_labels',
                                    interface=Fn(function=read_vtk,
                                                 input_names=['input_vtk',
                                                              'return_first',
                                                              'return_array'],
                                                 output_names=['faces',
                                                               'lines',
                                                               'indices',
                                                               'points',
                                                               'npoints',
                                                               'scalars',
                                                               'scalar_names',
                                                               'input_vtk']))
            SurfLabelFlow.add_nodes([ManualSurfLabels])
            mbFlow.connect(SurfaceAtlas, 'atlas_file',
                           SurfLabelFlow, 'Manual_surface_labels.input_vtk')
            ManualSurfLabels.inputs.return_first = 'True'
            ManualSurfLabels.inputs.return_array = 'False'
            plug = 'Manual_surface_labels.input_vtk'
            plug1 = ManualSurfLabels
            plug2 = 'input_vtk'

        # ====================================================================
        # Convert surface label numbers to volume DKT31 label numbers
        # ====================================================================
        ReindexLabels = Node(name='Reindex_labels',
                             interface=Fn(function=relabel_surface,
                                          input_names=['vtk_file',
                                                       'hemi',
                                                       'old_labels',
                                                       'new_labels',
                                                       'erase_remaining',
                                                       'erase_labels',
                                                       'erase_value',
                                                       'output_file'],
                                          output_names=['output_file']))
        SurfLabelFlow.add_nodes([ReindexLabels])
        SurfLabelFlow.connect(plug1, plug2, ReindexLabels, 'vtk_file')
        mbFlow.connect(InputHemis, 'hemi',
                       SurfLabelFlow, 'Reindex_labels.hemi')
        ReindexLabels.inputs.old_labels = dkt.DKT31_numbers
        ReindexLabels.inputs.new_labels = []
        ReindexLabels.inputs.erase_remaining = True
        ReindexLabels.inputs.erase_labels = [0]
        ReindexLabels.inputs.erase_value = -1
        ReindexLabels.inputs.output_file = ''
        if save_all:
            mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                           Sink, 'labels.@surface')

    # ========================================================================
    #
    #   Surface shape measurements
    #
    # ========================================================================
    if do_shapes:
        WholeSurfShapeFlow = Workflow(name='Surface_shapes')
        # --------------------------------------------------------------------
        # Measure surface area:
        # --------------------------------------------------------------------
        SurfaceArea = Node(name='Surface_area',
                           interface=Fn(function=area,
                                        input_names=['command',
                                                     'surface_file'],
                                        output_names=['area_file']))
        area_command = os.path.join(ccode_path, 'area', 'PointAreaMain')
        SurfaceArea.inputs.command = area_command
        # --------------------------------------------------------------------
        # Measure surface travel depth:
        # --------------------------------------------------------------------
        TravelDepth = Node(name='Travel_depth',
                           interface=Fn(function=travel_depth,
                                        input_names=['command',
                                                     'surface_file'],
                                        output_names=['depth_file']))
        WholeSurfShapeFlow.add_nodes([TravelDepth])
        TravelDepth.inputs.command = os.path.join(ccode_path,
                                                  'travel_depth',
                                                  'TravelDepthMain')
        # --------------------------------------------------------------------
        # Rescale surface travel depth:
        # --------------------------------------------------------------------
        if do_fundi:
            RescaleTravelDepth = Node(name='Rescale_travel_depth',
                                interface=Fn(function=rescale_by_neighborhood,
                                     input_names=['input_vtk',
                                                  'indices',
                                                  'nedges',
                                                  'p',
                                                  'set_max_to_1',
                                                  'save_file',
                                                  'output_filestring'],
                                     output_names=['rescaled_scalars',
                                                   'rescaled_scalars_file']))
            WholeSurfShapeFlow.add_nodes([RescaleTravelDepth])
            WholeSurfShapeFlow.connect(TravelDepth, 'depth_file',
                                       RescaleTravelDepth, 'input_vtk')
            RescaleTravelDepth.inputs.indices = []
            RescaleTravelDepth.inputs.nedges = 10
            RescaleTravelDepth.inputs.p = 99
            RescaleTravelDepth.inputs.set_max_to_1 = True
            RescaleTravelDepth.inputs.save_file = True
            RescaleTravelDepth.inputs.output_filestring = \
                'travel_depth_rescaled'
        # --------------------------------------------------------------------
        # Measure surface geodesic depth:
        # --------------------------------------------------------------------
        GeodesicDepth = Node(name='Geodesic_depth',
                             interface=Fn(function=geodesic_depth,
                                          input_names=['command',
                                                       'surface_file'],
                                          output_names=['depth_file']))
        GeodesicDepth.inputs.command = os.path.join(ccode_path,
                                                    'geodesic_depth',
                                                    'GeodesicDepthMain')
        # --------------------------------------------------------------------
        # Measure surface curvature:
        # --------------------------------------------------------------------
        CurvNode = Node(name='Curvature',
                        interface=Fn(function=curvature,
                             input_names=['command',
                                          'method',
                                          'arguments',
                                          'surface_file'],
                             output_names=['mean_curvature_file',
                                           'gauss_curvature_file',
                                           'max_curvature_file',
                                           'min_curvature_file',
                                           'min_curvature_vector_file']))
        CurvNode.inputs.command = os.path.join(ccode_path,
                                               'curvature',
                                               'CurvatureMain')
        CurvNode.inputs.method = 2
        CurvNode.inputs.arguments = '-n 0.7'
        # --------------------------------------------------------------------
        # Convert FreeSurfer surface measures to VTK:
        # --------------------------------------------------------------------
        if do_freesurfer_curvature:
            ConvexNode = Node(name='Curv_to_vtk',
                              interface=Fn(function=freesurfer_curvature_to_vtk,
                                           input_names=['surface_file',
                                                        'vtk_file',
                                                        'output_vtk'],
                                           output_names=['output_vtk']))
            WholeSurfShapeFlow.add_nodes([ConvexNode])
            mbFlow.connect(Surf, 'freesurfer_curvature_files',
                           WholeSurfShapeFlow, 'Curv_to_vtk.surface_file')
            mbFlow.connect(ConvertSurf, 'output_vtk',
                           WholeSurfShapeFlow, 'Curv_to_vtk.vtk_file')
            ConvexNode.inputs.output_vtk = ''
            if save_all:
                mbFlow.connect(WholeSurfShapeFlow, 'Curv_to_vtk.output_vtk',
                               Sink, 'shapes.@freesurfer_curvature')
        if do_freesurfer_sulc:
            ConvexNode = Node(name='Sulc_to_vtk',
                              interface=Fn(function=freesurfer_curvature_to_vtk,
                                           input_names=['surface_file',
                                                        'vtk_file',
                                                        'output_vtk'],
                                           output_names=['output_vtk']))
            WholeSurfShapeFlow.add_nodes([ConvexNode])
            mbFlow.connect(Surf, 'freesurfer_sulc_files',
                           WholeSurfShapeFlow, 'Sulc_to_vtk.surface_file')
            mbFlow.connect(ConvertSurf, 'output_vtk',
                           WholeSurfShapeFlow, 'Sulc_to_vtk.vtk_file')
            ConvexNode.inputs.output_vtk = ''
            if save_all:
                mbFlow.connect(WholeSurfShapeFlow, 'Sulc_to_vtk.output_vtk',
                               Sink, 'shapes.@freesurfer_sulc')
        if do_freesurfer_thickness:
            ThickNode = Node(name='Thickness_to_vtk',
                             interface=Fn(function=freesurfer_curvature_to_vtk,
                                          input_names=['surface_file',
                                                       'vtk_file',
                                                       'output_vtk'],
                                          output_names=['output_vtk']))
            WholeSurfShapeFlow.add_nodes([ThickNode])
            mbFlow.connect(Surf, 'freesurfer_thickness_files',
                           WholeSurfShapeFlow,
                           'Thickness_to_vtk.surface_file')
            mbFlow.connect(ConvertSurf, 'output_vtk',
                           WholeSurfShapeFlow, 'Thickness_to_vtk.vtk_file')
            ThickNode.inputs.output_vtk = ''
            if save_all:
                mbFlow.connect(WholeSurfShapeFlow, 'Thickness_to_vtk.output_vtk',
                               Sink, 'shapes.@freesurfer_thickness')
        # --------------------------------------------------------------------
        # Connect nodes:
        # --------------------------------------------------------------------
        WholeSurfShapeFlow.add_nodes([SurfaceArea, GeodesicDepth, CurvNode])
        if do_input_vtk:
            mbFlow.connect([(Surf, WholeSurfShapeFlow,
                             [('surface_files','Surface_area.surface_file'),
                              ('surface_files','Travel_depth.surface_file'),
                              ('surface_files','Geodesic_depth.surface_file'),
                              ('surface_files','Curvature.surface_file')])])
        else:
            mbFlow.connect([(ConvertSurf, WholeSurfShapeFlow,
                               [('output_vtk', 'Surface_area.surface_file'),
                                ('output_vtk', 'Travel_depth.surface_file'),
                                ('output_vtk', 'Geodesic_depth.surface_file'),
                                ('output_vtk', 'Curvature.surface_file')])])
        if save_all:
            mbFlow.connect([(WholeSurfShapeFlow, Sink,
               [('Surface_area.area_file', 'shapes.@surface_area'),
                ('Travel_depth.depth_file', 'shapes.@travel_depth'),
                ('Geodesic_depth.depth_file', 'shapes.@geodesic_depth'),
                ('Curvature.mean_curvature_file', 'shapes.@mean_curvature')])])

    # ========================================================================
    #
    #   Surface feature extraction
    #
    # ========================================================================
    if do_sulci or do_fundi:
        SurfFeatureFlow = Workflow(name='Surface_features')

        # ====================================================================
        # Folds and sulci
        # ====================================================================
        if do_sulci or do_fundi:
            # ----------------------------------------------------------------
            # Folds:
            # ----------------------------------------------------------------
            FoldsNode = Node(name='Folds',
                             interface=Fn(function=extract_folds,
                                          input_names=['depth_file',
                                                       'min_vertices',
                                                       'min_fold_size',
                                                       'do_fill_holes',
                                                       'min_hole_depth',
                                                       'save_file'],
                                          output_names=['folds',
                                                        'n_folds',
                                                        'depth_threshold',
                                                        'bins',
                                                        'bin_edges',
                                                        'folds_file']))
            SurfFeatureFlow.add_nodes([FoldsNode])
            mbFlow.connect(WholeSurfShapeFlow, 'Travel_depth.depth_file',
                           SurfFeatureFlow, 'Folds.depth_file')
            FoldsNode.inputs.min_vertices = 10000
            FoldsNode.inputs.min_fold_size = 50
            FoldsNode.inputs.do_fill_holes = False
            FoldsNode.inputs.min_hole_depth = 0.001
            FoldsNode.inputs.save_file = True
            if save_folds and save_all:
               mbFlow.connect(SurfFeatureFlow, 'Folds.folds_file',
                              Sink, 'features.@folds')
            # ----------------------------------------------------------------
            # Sulci:
            # ----------------------------------------------------------------
            SulciNode = Node(name='Sulci',
                             interface=Fn(function=extract_sulci,
                                          input_names=['labels_file',
                                                       'folds_or_file',
                                                       'hemi',
                                                       'min_boundary',
                                                       'sulcus_names'],
                                          output_names=['sulci',
                                                        'n_sulci',
                                                        'sulci_file']))
            SurfFeatureFlow.add_nodes([SulciNode])
            mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                           SurfFeatureFlow, 'Sulci.labels_file')
            SurfFeatureFlow.connect(FoldsNode, 'folds',
                                    SulciNode, 'folds_or_file')
            mbFlow.connect(InputHemis, 'hemi', SurfFeatureFlow, 'Sulci.hemi')
            SulciNode.inputs.min_boundary = 1
            SulciNode.inputs.sulcus_names = dkt.sulcus_names
            if save_all:
                mbFlow.connect(SurfFeatureFlow, 'Sulci.sulci_file',
                               Sink, 'features.@sulci')

        # ====================================================================
        # Fundi
        # ====================================================================
        if do_fundi:
            # ----------------------------------------------------------------
            # Extract a fundus per fold:
            # ----------------------------------------------------------------
            FoldFundi = Node(name='Fundus_per_fold',
                             interface=Fn(function=extract_fundi,
                                  input_names=['folds',
                                               'curv_file',
                                               'depth_file',
                                               'min_separation',
                                               'erode_ratio',
                                               'erode_min_size',
                                               'save_file'],
                                  output_names=['fundus_per_fold',
                                                'n_fundi_in_folds',
                                                'fundus_per_fold_file']))
            SurfFeatureFlow.connect(FoldsNode, 'folds', FoldFundi, 'folds')
            mbFlow.connect([(WholeSurfShapeFlow, SurfFeatureFlow,
                           [('Curvature.mean_curvature_file',
                             'Fundus_per_fold.curv_file'),
                            ('Rescale_travel_depth.rescaled_scalars_file',
                             'Fundus_per_fold.depth_file')])])
            FoldFundi.inputs.min_separation = 10
            FoldFundi.inputs.erode_ratio = 0.10
            FoldFundi.inputs.erode_min_size = 10
            FoldFundi.inputs.save_file = False
            #if save_all:
            #    mbFlow.connect(SurfFeatureFlow,
            #                   'Fundus_per_fold.fundus_per_fold_file',
            #                   Sink, 'features.@fundus_per_fold')

            if do_smooth_fundi:
                # ------------------------------------------------------------
                # Compute likelihoods for smoothing fundi:
                # ------------------------------------------------------------
                LikelihoodNode = Node(name='Likelihood',
                    interface=Fn(function=compute_likelihood,
                                 input_names=['trained_file',
                                              'depth_file',
                                              'curvature_file',
                                              'folds',
                                              'save_file'],
                                 output_names=['likelihoods',
                                               'likelihoods_file']))
                SurfFeatureFlow.add_nodes([LikelihoodNode])
                border_params_file = \
                    'depth_curv_border_nonborder_parameters.pkl'
                border_params_path = fetch_check_data(border_params_file, url,
                                                   hashes, cache_env, cache)
                LikelihoodNode.inputs.trained_file = border_params_path
                mbFlow.connect([(WholeSurfShapeFlow, SurfFeatureFlow,
                    [('Rescale_travel_depth.rescaled_scalars_file',
                      'Likelihood.depth_file'),
                     ('Curvature.mean_curvature_file',
                      'Likelihood.curvature_file')])])
                SurfFeatureFlow.connect(FoldsNode, 'folds',
                                        LikelihoodNode, 'folds')
                LikelihoodNode.inputs.save_file = True
                #if save_all:
                #    mbFlow.connect(SurfFeatureFlow, 'Likelihood.likelihoods_file',
                #                   Sink, 'features.@likelihoods')
                # ------------------------------------------------------------
                # Smooth fundi:
                # ------------------------------------------------------------
                SmoothFundi = Node(name='Smooth_fundi',
                                   interface=Fn(function=smooth_skeleton,
                                        input_names=['skeletons',
                                                     'bounds',
                                                     'vtk_file',
                                                     'likelihoods',
                                                     'wN_max',
                                                     'erode_again',
                                                     'save_file'],
                                        output_names=['smooth_skeletons',
                                                      'n_skeletons',
                                                      'skeletons_file']))
                SurfFeatureFlow.connect(FoldFundi, 'fundus_per_fold',
                                        SmoothFundi, 'skeletons')
                SurfFeatureFlow.connect(FoldsNode, 'folds',
                                        SmoothFundi, 'bounds')
                mbFlow.connect(WholeSurfShapeFlow,
                               'Curvature.mean_curvature_file',
                               SurfFeatureFlow, 'Smooth_fundi.vtk_file')
                SurfFeatureFlow.connect(LikelihoodNode, 'likelihoods',
                                        SmoothFundi, 'likelihoods')
                SmoothFundi.inputs.wN_max = 1.0
                SmoothFundi.inputs.erode_again = False
                SmoothFundi.inputs.save_file = True
                if save_all:
                    mbFlow.connect(SurfFeatureFlow, 'Smooth_fundi.skeletons_file',
                                   Sink, 'features.@smooth_fundi')

            # ----------------------------------------------------------------
            # Segment a fundus per sulcus:
            # ----------------------------------------------------------------
            SulcusFundi = Node(name='Fundus_per_sulcus',
                               interface=Fn(function=segment_fundi,
                                    input_names=['fundus_per_fold',
                                                 'sulci',
                                                 'vtk_file',
                                                 'save_file'],
                                    output_names=['fundus_per_sulcus',
                                                  'n_fundi',
                                                  'fundus_per_sulcus_file']))
            if do_smooth_fundi:
                SurfFeatureFlow.connect(SmoothFundi, 'smooth_skeletons',
                                        SulcusFundi, 'fundus_per_fold')
            else:
                SurfFeatureFlow.connect(FoldFundi, 'fundus_per_fold',
                                        SulcusFundi, 'fundus_per_fold')
            SurfFeatureFlow.connect(SulciNode, 'sulci', SulcusFundi, 'sulci')
            mbFlow.connect(WholeSurfShapeFlow,
                           'Curvature.mean_curvature_file',
                           SurfFeatureFlow, 'Fundus_per_sulcus.vtk_file')
            SulcusFundi.inputs.save_file = True
            if save_all:
                mbFlow.connect(SurfFeatureFlow,
                               'Fundus_per_sulcus.fundus_per_sulcus_file',
                               Sink, 'features.@fundus_per_sulcus')

    # ========================================================================
    #
    #   Surface feature shapes
    #
    # ========================================================================
    if do_shapes:
        SurfFeatureShapeFlow = Workflow(name='Surface_feature_shapes')
        # ====================================================================
        # Compute Laplace-Beltrami spectra
        # ====================================================================
        if do_spectra:
            # ----------------------------------------------------------------
            # Measure spectra of labeled regions:
            # ----------------------------------------------------------------
            SpectraLabels = Node(name='Spectra_labels',
                                 interface=Fn(function=spectrum_per_label,
                                              input_names=['vtk_file',
                                                           'spectrum_size',
                                                           'exclude_labels',
                                                           'normalization',
                                                           'area_file',
                                                           'largest_segment'],
                                              output_names=['spectrum_lists',
                                                            'label_list']))
            SurfFeatureShapeFlow.add_nodes([SpectraLabels])
            mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                           SurfFeatureShapeFlow, 'Spectra_labels.vtk_file')
            SpectraLabels.inputs.spectrum_size = spectra
            SpectraLabels.inputs.exclude_labels = [0]
            SpectraLabels.inputs.normalization = "area"
            SpectraLabels.inputs.area_file = ""
            SpectraLabels.inputs.largest_segment = True
            mbFlow.connect(WholeSurfShapeFlow, 'Surface_area.area_file',
                           SurfFeatureShapeFlow, 'Spectra_labels.area_file')
            # ----------------------------------------------------------------
            # Compute spectra of sulci:
            # ----------------------------------------------------------------
            if do_sulci:
                SpectraSulci = SpectraLabels.clone('Spectra_sulci')
                SurfFeatureShapeFlow.add_nodes([SpectraSulci])
                mbFlow.connect(SurfFeatureFlow, 'Sulci.sulci_file',
                               SurfFeatureShapeFlow, 'Spectra_sulci.vtk_file')
                SpectraSulci.inputs.exclude_labels = [-1]

        # ====================================================================
        # Compute Zernike moments
        # ====================================================================
        if do_moments:
            # ----------------------------------------------------------------
            # Measure Zernike moments of labeled regions:
            # ----------------------------------------------------------------
            ZernikeLabels = Node(name='Zernike_labels',
                 interface=Fn(function=zernike_moments_per_label,
                              input_names=['vtk_file',
                                           'order',
                                           'exclude_labels',
                                           'scale_input',
                                           'decimate_fraction',
                                           'decimate_smooth'],
                              output_names=['descriptors_lists',
                                            'label_list']))
            SurfFeatureShapeFlow.add_nodes([ZernikeLabels])
            mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                           SurfFeatureShapeFlow, 'Zernike_labels.vtk_file')
            ZernikeLabels.inputs.order = moments
            ZernikeLabels.inputs.exclude_labels = [0]
            ZernikeLabels.inputs.scale_input = True
            ZernikeLabels.inputs.decimate_fraction = 0
            ZernikeLabels.inputs.decimate_smooth = 0
            # ----------------------------------------------------------------
            # Compute Zernike moments of sulci:
            # ----------------------------------------------------------------
            if do_sulci:
                ZernikeSulci = ZernikeLabels.clone('Zernike_sulci')
                SurfFeatureShapeFlow.add_nodes([ZernikeSulci])
                mbFlow.connect(SurfFeatureFlow, 'Sulci.sulci_file',
                               SurfFeatureShapeFlow, 'Zernike_sulci.vtk_file')
                ZernikeSulci.inputs.exclude_labels = [-1]

    # ========================================================================
    #
    #   Surface feature shape tables
    #
    # ========================================================================
    if do_shapes:
        # --------------------------------------------------------------------
        # Surface feature shape tables: labels, sulci, fundi:
        # --------------------------------------------------------------------
        ShapeTables = Node(name='Shape_tables',
                           interface=Fn(function=write_shape_stats,
                                input_names=['labels_or_file',
                                             'sulci',
                                             'fundi',
                                             'affine_transform_files',
                                             'inverse_booleans',
                                             'transform_format',
                                             'area_file',
                                             'normalize_by_area',
                                             'mean_curvature_file',
                                             'travel_depth_file',
                                             'geodesic_depth_file',
                                             'freesurfer_thickness_file',
                                             'freesurfer_curvature_file',
                                             'freesurfer_sulc_file',
                                             'labels_spectra',
                                             'labels_spectra_IDs',
                                             'sulci_spectra',
                                             'sulci_spectra_IDs',
                                             'labels_zernike',
                                             'labels_zernike_IDs',
                                             'sulci_zernike',
                                             'sulci_zernike_IDs',
                                             'exclude_labels'],
                                output_names=['label_table',
                                              'sulcus_table',
                                              'fundus_table']))
        mbFlow.add_nodes([ShapeTables])
        ShapeTables.inputs.labels_or_file = []
        ShapeTables.inputs.sulci = []
        ShapeTables.inputs.fundi = []
        ShapeTables.inputs.affine_transform_files = None
        ShapeTables.inputs.inverse_booleans = None
        ShapeTables.inputs.transform_format = None
        ShapeTables.inputs.freesurfer_thickness_file = ''
        ShapeTables.inputs.freesurfer_curvature_file = ''
        ShapeTables.inputs.freesurfer_sulc_file = ''
        ShapeTables.inputs.labels_spectra = []
        ShapeTables.inputs.sulci_spectra = []
        ShapeTables.inputs.labels_spectra_IDs = []
        ShapeTables.inputs.sulci_spectra_IDs = []
        ShapeTables.inputs.labels_zernike = []
        ShapeTables.inputs.sulci_zernike = []
        ShapeTables.inputs.labels_zernike_IDs = []
        ShapeTables.inputs.sulci_zernike_IDs = []
        if do_label:
            mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                           ShapeTables, 'labels_or_file')
        if do_sulci:
            mbFlow.connect(SurfFeatureFlow, 'Sulci.sulci',
                           ShapeTables, 'sulci')
        if do_fundi:
            mbFlow.connect(SurfFeatureFlow,
                           'Fundus_per_sulcus.fundus_per_sulcus',
                           ShapeTables, 'fundi')
        if use_ants:
            mbFlow.connect(AffineFileList, 'string_list',
                           ShapeTables, 'affine_transform_files')
            ShapeTables.inputs.inverse_booleans = inverse_Booleans
            ShapeTables.inputs.transform_format = 'itk'
        ShapeTables.inputs.normalize_by_area = False
        mbFlow.connect([(WholeSurfShapeFlow, ShapeTables,
                   [('Surface_area.area_file', 'area_file'),
                    ('Curvature.mean_curvature_file', 'mean_curvature_file'),
                    ('Travel_depth.depth_file', 'travel_depth_file'),
                    ('Geodesic_depth.depth_file', 'geodesic_depth_file')])])
        if do_freesurfer_thickness:
            mbFlow.connect(WholeSurfShapeFlow, 'Thickness_to_vtk.output_vtk',
                           ShapeTables, 'freesurfer_thickness_file')
        if do_freesurfer_curvature:
            mbFlow.connect(WholeSurfShapeFlow, 'Curv_to_vtk.output_vtk',
                           ShapeTables, 'freesurfer_curvature_file')
        if do_freesurfer_sulc:
            mbFlow.connect(WholeSurfShapeFlow, 'Sulc_to_vtk.output_vtk',
                           ShapeTables, 'freesurfer_sulc_file')

        # Laplace-Beltrami spectra:
        if do_spectra:
            mbFlow.connect(SurfFeatureShapeFlow,
                           'Spectra_labels.spectrum_lists',
                           ShapeTables, 'labels_spectra')
            mbFlow.connect(SurfFeatureShapeFlow, 'Spectra_labels.label_list',
                           ShapeTables, 'labels_spectra_IDs')
            if do_sulci:
                mbFlow.connect(SurfFeatureShapeFlow,
                               'Spectra_sulci.spectrum_lists',
                               ShapeTables, 'sulci_spectra')
                mbFlow.connect(SurfFeatureShapeFlow,
                               'Spectra_sulci.label_list',
                               ShapeTables, 'sulci_spectra_IDs')
            else:
                ShapeTables.inputs.sulci_spectra = []
                ShapeTables.inputs.sulci_spectra_IDs = []

        # Zernike moments:
        if do_moments:
            mbFlow.connect(SurfFeatureShapeFlow,
                           'Zernike_labels.descriptors_lists',
                           ShapeTables, 'labels_zernike')
            mbFlow.connect(SurfFeatureShapeFlow, 'Zernike_labels.label_list',
                           ShapeTables, 'labels_zernike_IDs')
            if do_sulci:
                mbFlow.connect(SurfFeatureShapeFlow,
                               'Zernike_sulci.descriptors_lists',
                               ShapeTables, 'sulci_zernike')
                mbFlow.connect(SurfFeatureShapeFlow,
                               'Zernike_sulci.label_list',
                               ShapeTables, 'sulci_zernike_IDs')
            else:
                ShapeTables.inputs.sulci_zernike = []
                ShapeTables.inputs.sulci_zernike_IDs = []

        ShapeTables.inputs.exclude_labels = [-1]
        mbFlow.connect(ShapeTables, 'label_table', Sink, 'tables.@labels')
        if do_sulci:
            mbFlow.connect(ShapeTables, 'sulcus_table', Sink, 'tables.@sulci')
        if do_fundi:
            mbFlow.connect(ShapeTables, 'fundus_table', Sink, 'tables.@fundi')
        # --------------------------------------------------------------------
        # Vertex measures table:
        # --------------------------------------------------------------------
        if do_points:
            VertexTable = Node(name='Vertex_table',
                               interface=Fn(function=write_vertex_measures,
                                    input_names=['output_table',
                                                 'labels_or_file',
                                                 'sulci',
                                                 'fundi',
                                                 'affine_transform_files',
                                                 'inverse_booleans',
                                                 'transform_format',
                                                 'area_file',
                                                 'mean_curvature_file',
                                                 'travel_depth_file',
                                                 'geodesic_depth_file',
                                                 'freesurfer_thickness_file',
                                                 'freesurfer_curvature_file',
                                                 'freesurfer_sulc_file'],
                                    output_names=['output_table']))
            mbFlow.add_nodes([VertexTable])
            VertexTable.inputs.output_table = ''
            VertexTable.inputs.labels_or_file = []
            VertexTable.inputs.sulci = []
            VertexTable.inputs.fundi = []
            VertexTable.inputs.affine_transform_files = None
            VertexTable.inputs.inverse_booleans = None
            VertexTable.inputs.transform_format = None
            VertexTable.inputs.freesurfer_thickness_file = ''
            VertexTable.inputs.freesurfer_curvature_file = ''
            VertexTable.inputs.freesurfer_sulc_file = ''
            if do_label:
                mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                               VertexTable, 'labels_or_file')
            if do_sulci:
                mbFlow.connect(SurfFeatureFlow, 'Sulci.sulci',
                               VertexTable, 'sulci')
            if do_fundi:
                mbFlow.connect(SurfFeatureFlow,
                               'Fundus_per_sulcus.fundus_per_sulcus',
                               VertexTable, 'fundi')

            if use_ants:
                mbFlow.connect(AffineFileList, 'string_list',
                               VertexTable, 'affine_transform_files')
                VertexTable.inputs.inverse_booleans = inverse_Booleans
                VertexTable.inputs.transform_format = 'itk'
            mbFlow.connect([(WholeSurfShapeFlow, VertexTable,
                               [('Surface_area.area_file','area_file'),
                                ('Travel_depth.depth_file',
                                 'travel_depth_file'),
                                ('Geodesic_depth.depth_file',
                                 'geodesic_depth_file'),
                                ('Curvature.mean_curvature_file',
                                 'mean_curvature_file')])])
            if do_freesurfer_thickness:
                mbFlow.connect(WholeSurfShapeFlow,
                               'Thickness_to_vtk.output_vtk',
                               VertexTable, 'freesurfer_thickness_file')
            if do_freesurfer_curvature:
                mbFlow.connect(WholeSurfShapeFlow,
                               'Curv_to_vtk.output_vtk',
                               VertexTable, 'freesurfer_curvature_file')
            if do_freesurfer_sulc:
                mbFlow.connect(WholeSurfShapeFlow,
                               'Sulc_to_vtk.output_vtk',
                               VertexTable, 'freesurfer_sulc_file')
            mbFlow.connect(VertexTable, 'output_table',
                           Sink, 'tables.@vertices')

        # --------------------------------------------------------------------
        # Apply affine transform to surface coordinates:
        # --------------------------------------------------------------------
        if use_ants and do_surfaces_in_mni:
            TransformVTK = Node(name='Transform_surface_points',
                                interface=Fn(function=apply_affine_transforms,
                                             input_names=['transform_files',
                                                          'inverse_booleans',
                                                          'transform_format',
                                                          'vtk_or_points',
                                                          'vtk_file_stem'],
                                             output_names=['affine_points',
                                                           'output_file']))
            mbFlow.add_nodes([TransformVTK])
            mbFlow.connect(AffineFileList, 'string_list',
                           TransformVTK, 'transform_files')
            mbFlow.connect(TravelDepth, 'depth_file',
                           TransformVTK, 'vtk_or_points')
            TransformVTK.inputs.inverse_booleans = inverse_Booleans
            TransformVTK.inputs.transform_format = 'itk'
            TransformVTK.inputs.vtk_file_stem = 'affine_'
            if save_all:
                mbFlow.connect(TransformVTK, 'output_file',
                               Sink, 'features.@surface_in_MNI152')


# ============================================================================
# ----------------------------------------------------------------------------
#
#   Volume workflows
#
# ----------------------------------------------------------------------------
# ============================================================================
if do_label and not args.no_volumes:

    # ========================================================================
    #
    #   Location and structure of FreeSurfer volume inputs
    #
    # ========================================================================
    # ------------------------------------------------------------------------
    # Original image (.mgz) for converting from conformal (below):
    # ------------------------------------------------------------------------
    mghOrig = Node(name='mgh_orig',
                   interface=DataGrabber(infields=['subject'],
                                         outfields=['mgh_orig'],
                                         sort_filelist=False))
    mghOrig.inputs.base_directory = subjects_dir
    second_scan = os.path.join(subjects_dir, 'mri/orig/002.mgz')
    if os.path.exists(second_scan):
        raise(IOError("Mindboggle can not make use of FreeSurfer "
                      "output with more than one scan: " + second_scan))
    mghOrig.inputs.template = '%s/mri/orig/001.mgz'
    mghOrig.inputs.template_args['mgh_orig'] = [['subject']]
    mghOrig.inputs.subject = subject
    # --------------------------------------------------------------------
    # Convert FreeSurfer mgh conformal file to nifti format:
    # --------------------------------------------------------------------
    MGH2Nifti = Node(name='mgh_to_nifti',
                     interface=Fn(function=convert2nii,
                                  input_names=['input_file',
                                               'reference_file',
                                               'output_file',
                                               'interp'],
                                  output_names=['output_file']))
    mbFlow.connect(mghOrig, 'mgh_orig', MGH2Nifti, 'input_file')
    mbFlow.connect(mghOrig, 'mgh_orig', MGH2Nifti, 'reference_file')
    MGH2Nifti.inputs.output_file = ''
    MGH2Nifti.inputs.interp = 'continuous'
    # ------------------------------------------------------------------------
    # Use own whole-brain nifti label volume:
    # ------------------------------------------------------------------------
    if do_input_fs_labels:
        labelsNifti = Node(name='labels_nifti',
                           interface=DataGrabber(infields=['subject'],
                                                 outfields=['labels'],
                                                 sort_filelist=False))
        labelsNifti.inputs.base_directory = subjects_dir
        labelsNifti.inputs.template = '%s/mri/' + volume_labels+'.nii.gz'
        labelsNifti.inputs.template_args['labels'] = [['subject']]
        labelsNifti.inputs.subject = subject
    # ------------------------------------------------------------------------
    # Convert FreeSurfer whole-brain label volume to nifti format:
    # ------------------------------------------------------------------------
    else:
        # --------------------------------------------------------------------
        #  label volume:
        # --------------------------------------------------------------------
        labelsMGH = Node(name='labels_mgh',
                         interface=DataGrabber(infields=['subject'],
                                               outfields=['labels'],
                                               sort_filelist=False))
        labelsMGH.inputs.base_directory = subjects_dir
        labelsMGH.inputs.template = '%s/mri/' + volume_labels+'.mgz'
        labelsMGH.inputs.template_args['labels'] = [['subject']]
        labelsMGH.inputs.subject = subject
        # --------------------------------------------------------------------
        # Convert FreeSurfer mgh conformal file to nifti format:
        # --------------------------------------------------------------------
        labelsMGH2Nifti = Node(name='labels_mgh_to_nifti',
                           interface=Fn(function=convert2nii,
                                        input_names=['input_file',
                                                     'reference_file',
                                                     'output_file',
                                                     'interp'],
                                        output_names=['output_file']))
        mbFlow.connect(labelsMGH, 'labels', labelsMGH2Nifti, 'input_file')
        mbFlow.connect(mghOrig, 'mgh_orig', labelsMGH2Nifti, 'reference_file')
        labelsMGH2Nifti.inputs.output_file = ''
        labelsMGH2Nifti.inputs.interp = 'nearest'
        #if save_all:
        #    mbFlow.connect(labelsMGH2Nifti, 'output_file',
        #                   Sink, 'labels.@freesurfer')

    # ========================================================================
    #
    #   Volume labels
    #
    # ========================================================================
    VolLabelFlow = Workflow(name='Volume_labels')

    # ------------------------------------------------------------------------
    # Extract FreeSurfer cerebellum labels:
    # ------------------------------------------------------------------------
    FScerebellum = Node(name='Extract_FreeSurfer_cerebella',
                        interface=Fn(function=keep_volume_labels,
                                     input_names=['input_file',
                                                  'labels_to_keep',
                                                  'output_file',
                                                  'second_file'],
                                     output_names=['output_file']))
    VolLabelFlow.add_nodes([FScerebellum])
    if do_input_fs_labels:
        mbFlow.connect(labelsNifti, 'labels', VolLabelFlow,
                       'Extract_FreeSurfer_cerebella.input_file')
    else:
        mbFlow.connect(labelsMGH2Nifti, 'output_file', VolLabelFlow,
                       'Extract_FreeSurfer_cerebella.input_file')
    FScerebellum.inputs.labels_to_keep = dkt.cerebellum_numbers
    FScerebellum.inputs.output_file = ''
    FScerebellum.inputs.second_file = ''

    # ========================================================================
    # Combine FreeSurfer and ANTs cerebrum segmentation volumes
    # ========================================================================
    if not my_segments:
        # --------------------------------------------------------------------
        # Extract FreeSurfer cerebrum labels:
        # --------------------------------------------------------------------
        FScerebrum = Node(name='Extract_FreeSurfer_cerebra',
                          interface=Fn(function=keep_volume_labels,
                                       input_names=['input_file',
                                                    'labels_to_keep',
                                                    'output_file',
                                                    'second_file'],
                                       output_names=['output_file']))
        VolLabelFlow.add_nodes([FScerebrum])
        if do_input_fs_labels:
            mbFlow.connect(labelsNifti, 'labels', VolLabelFlow,
                           'Extract_FreeSurfer_cerebra.input_file')
        else:
            mbFlow.connect(labelsMGH2Nifti, 'output_file', VolLabelFlow,
                           'Extract_FreeSurfer_cerebra.input_file')
        labels_to_segment = dkt.cerebrum_cortex_numbers + \
                            dkt.cerebrum_noncortex_numbers + \
                            dkt.brainstem_numbers + \
                            dkt.extra_numbers
        FScerebrum.inputs.labels_to_keep = labels_to_segment
        FScerebrum.inputs.output_file = ''
        FScerebrum.inputs.second_file = ''
        # --------------------------------------------------------------------
        # Convert FreeSurfer cerebrum labels to non/cortex segments:
        # --------------------------------------------------------------------
        FSsegments = Node(name='FreeSurfer_cerebrum_labels_to_segments',
                          interface=Fn(function=relabel_volume,
                                       input_names=['input_file',
                                                    'old_labels',
                                                    'new_labels',
                                                    'output_file'],
                                       output_names=['output_file']))
        VolLabelFlow.add_nodes([FSsegments])
        VolLabelFlow.connect(FScerebrum, 'output_file',
                             FSsegments, 'input_file')
        FSsegments.inputs.old_labels = labels_to_segment
        FSsegments.inputs.new_labels = \
            [2 for x in dkt.cerebrum_cortex_numbers] + \
            [3 for x in dkt.cerebrum_noncortex_numbers] + \
            [3 for x in dkt.brainstem_numbers] + \
            [3 for x in dkt.extra_numbers]
        FSsegments.inputs.output_file = ''
        # --------------------------------------------------------------------
        # Convert ANTs Atropos-segmented volume to non/cortex segments:
        # --------------------------------------------------------------------
        if use_ants:
            ANTsSegments = FSsegments.clone('Relabel_ANTs_segments')
            VolLabelFlow.add_nodes([ANTsSegments])
            mbFlow.connect(FetchANTs, 'segments', VolLabelFlow,
                           'Relabel_ANTs_segments.input_file')
            ANTsSegments.inputs.old_labels = [1, 4]
            ANTsSegments.inputs.new_labels = [0, 3]
            ANTsSegments.inputs.output_file = ''
            # ----------------------------------------------------------------
            # Combine FreeSurfer and ANTs cerebrum segmentation volumes to
            # obtain a single cortex (2) and noncortex (3) segmentation file:
            # ----------------------------------------------------------------
            JoinSegs = Node(name='Combine_FreeSurfer_ANTs_cerebrum_segments',
                            interface=Fn(function=combine_2labels_in_2volumes,
                                         input_names=['file1',
                                                      'file2',
                                                      'label1',
                                                      'label2',
                                                      'output_file'],
                                         output_names=['output_file']))
            VolLabelFlow.add_nodes([JoinSegs])
            VolLabelFlow.connect(FSsegments, 'output_file', JoinSegs, 'file1')
            JoinSegs.inputs.out_dir = ''
            VolLabelFlow.connect(ANTsSegments, 'output_file',
                                 JoinSegs, 'file2')
            JoinSegs.inputs.label1 = 3
            JoinSegs.inputs.label2 = 2
            JoinSegs.inputs.output_file = ''
            # ----------------------------------------------------------------
            # Erase cerebrum that overlaps with FreeSurfer cerebellum:
            # ----------------------------------------------------------------
            if overwrite_cerebrum_with_cerebellum:
                RemoveCerebellum = Node(
                    name='Remove_cerebrum_cerebellum_overlap',
                    interface=Fn(function=remove_volume_labels,
                                 input_names=['input_file',
                                              'labels_to_remove',
                                              'output_file',
                                              'second_file'],
                                 output_names=['output_file']))
                VolLabelFlow.add_nodes([RemoveCerebellum])
                VolLabelFlow.connect(FScerebellum, 'output_file',
                                     RemoveCerebellum, 'input_file')
                RemoveCerebellum.inputs.labels_to_remove = \
                    dkt.cerebellum_numbers
                RemoveCerebellum.inputs.output_file = ''
                VolLabelFlow.connect(JoinSegs, 'output_file',
                                     RemoveCerebellum, 'second_file')

    # ========================================================================
    # Split segmented brain into two sides (without medial regions)
    # ========================================================================
    # if modify_surface_labels:
    #     # --------------------------------------------------------------------
    #     # Split brain by masking with left or right labels:
    #     # --------------------------------------------------------------------
    #     SplitBrainSegs = Node(name='Split_brain',
    #                           interface=Fn(function=split_brain,
    #                                        input_names=['image_file',
    #                                                     'label_file',
    #                                                     'left_labels',
    #                                                     'right_labels'],
    #                                        output_names=['left_brain',
    #                                                      'right_brain']))
    #     VolLabelFlow.add_nodes([SplitBrainSegs])
    #     if my_segments:
    #         SplitBrainSegs.inputs.image_file = my_segments
    #     else:
    #         if use_ants:
    #             if overwrite_cerebrum_with_cerebellum:
    #                 VolLabelFlow.connect(RemoveCerebellum, 'output_file',
    #                                      SplitBrainSegs, 'image_file')
    #             else:
    #                 VolLabelFlow.connect(JoinSegs, 'output_file',
    #                                      SplitBrainSegs, 'image_file')
    #         else:
    #             VolLabelFlow.connect(FSsegments, 'output_file',
    #                                  SplitBrainSegs, 'image_file')
    #     if do_input_fs_labels:
    #         mbFlow.connect(labelsNifti, 'labels', VolLabelFlow,
    #                        'Split_brain.label_file')
    #     else:
    #         mbFlow.connect(labelsMGH2Nifti, 'output_file', VolLabelFlow,
    #                        'Split_brain.label_file')
    #     SplitBrainSegs.inputs.left_labels = dkt.left_cerebrum_numbers
    #     SplitBrainSegs.inputs.right_labels = dkt.right_cerebrum_numbers

    # ========================================================================
    # Fill cerebrum segmentation volumes with FreeSurfer labels
    # ========================================================================
    # ------------------------------------------------------------------------
    # Extract cerebrum noncortical volume labels:
    # ------------------------------------------------------------------------
    FSnoncortex = Node(name='Extract_FreeSurfer_noncortex_labels',
                       interface=Fn(function=keep_volume_labels,
                                    input_names=['input_file',
                                                 'labels_to_keep',
                                                 'output_file',
                                                 'second_file'],
                                    output_names=['output_file']))
    VolLabelFlow.add_nodes([FSnoncortex])
    if do_input_fs_labels:
        mbFlow.connect(labelsNifti, 'labels', VolLabelFlow,
                       'Extract_FreeSurfer_noncortex_labels.input_file')
    else:
        mbFlow.connect(labelsMGH2Nifti, 'output_file', VolLabelFlow,
                       'Extract_FreeSurfer_noncortex_labels.input_file')
    labels_to_fill = dkt.cerebrum_noncortex_numbers + \
                     dkt.brainstem_numbers + \
                     dkt.extra_numbers
    FSnoncortex.inputs.labels_to_keep = labels_to_fill
    FSnoncortex.inputs.output_file = ''
    FSnoncortex.inputs.second_file = ''
    # ------------------------------------------------------------------------
    # Propagate FreeSurfer volume labels through noncortex:
    # ------------------------------------------------------------------------
    FSFillNoncortex = Node(name='Fill_noncortex_with_FreeSurfer_labels',
                           interface=Fn(function=PropagateLabelsThroughMask,
                                        input_names=['mask',
                                                     'labels',
                                                     'mask_index',
                                                     'output_file',
                                                     'binarize',
                                                     'stopvalue'],
                                        output_names=['output_file']))
    VolLabelFlow.add_nodes([FSFillNoncortex])
    if my_segments:
        FSFillNoncortex.inputs.mask = my_segments
    else:
        if use_ants:
            if overwrite_cerebrum_with_cerebellum:
                VolLabelFlow.connect(RemoveCerebellum, 'output_file',
                                     FSFillNoncortex, 'mask')
            else:
                VolLabelFlow.connect(JoinSegs, 'output_file',
                                     FSFillNoncortex, 'mask')
        else:
            VolLabelFlow.connect(FSsegments, 'output_file',
                                 FSFillNoncortex, 'mask')
    VolLabelFlow.connect(FSnoncortex, 'output_file',
                         FSFillNoncortex, 'labels')
    FSFillNoncortex.inputs.mask_index = 3
    FSFillNoncortex.inputs.output_file = ''
    FSFillNoncortex.inputs.binarize = False
    FSFillNoncortex.inputs.stopvalue = ''
    # ------------------------------------------------------------------------
    # Propagate FreeSurfer surface labels through whole-brain cortex:
    # ------------------------------------------------------------------------
    # if modify_surface_labels:
    #
    #     print('NOTE: Evaluate surface-to-volume label propagation'
    #           ' when surface label modification algorithm complete.')
    #
    #     # --------------------------------------------------------------------
    #     # Propagate surface labels through each hemisphere's cortex:
    #     # --------------------------------------------------------------------
    #     FSsurfFillCortex = Node(
    #                 name='Fill_cortex_with_FreeSurfer_surface_labels',
    #                 interface=Fn(function=fill_volume_with_surface_labels,
    #                              input_names=['hemi',
    #                                           'left_mask',
    #                                           'right_mask',
    #                                           'surface_files',
    #                                           'mask_index',
    #                                           'output_file',
    #                                           'binarize'],
    #                              output_names=['output_file']))
    #     VolLabelFlow.add_nodes([FSsurfFillCortex])
    #     mbFlow.connect(InputHemis, 'hemi', VolLabelFlow,
    #                    'Fill_cortex_with_FreeSurfer_surface_labels.hemi')
    #     VolLabelFlow.connect(SplitBrainSegs, 'left_brain',
    #                          FSsurfFillCortex, 'left_mask')
    #     VolLabelFlow.connect(SplitBrainSegs, 'right_brain',
    #                          FSsurfFillCortex, 'right_mask')
    #     mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
    #         VolLabelFlow,
    #         'Fill_cortex_with_FreeSurfer_surface_labels.surface_files')
    #     FSsurfFillCortex.inputs.mask_index = 2
    #     FSsurfFillCortex.inputs.output_file = ''
    #     FSsurfFillCortex.inputs.binarize = False
    #     # --------------------------------------------------------------------
    #     # Combine left and right cortical labels:
    #     # --------------------------------------------------------------------
    #     SplitHemiList = JoinNode(name='Split_hemisphere_list',
    #                              interface=Fn(function=split_list_pair,
    #                                           input_names=['List'],
    #                                           output_names=['element1',
    #                                                         'element2']),
    #                              joinsource="Input_hemispheres",
    #                              joinfield="List")
    #     LRcortex = Node(name='Combine_left_right_cortex_labels',
    #                     interface=Fn(function=ImageMath,
    #                                  input_names=['volume1',
    #                                               'volume2',
    #                                               'operator',
    #                                               'output_file'],
    #                                  output_names=['output_file']))
    #     VolLabelFlow.add_nodes([SplitHemiList, LRcortex])
    #     VolLabelFlow.connect(FSsurfFillCortex, 'output_file',
    #                          SplitHemiList, 'List')
    #     VolLabelFlow.connect(SplitHemiList, 'element1', LRcortex, 'volume1')
    #     VolLabelFlow.connect(SplitHemiList, 'element2', LRcortex, 'volume2')
    #     LRcortex.inputs.operator = '+'
    #     LRcortex.inputs.output_file = ''
    #else:
    # ------------------------------------------------------------------------
    # Propagate FreeSurfer volume labels through whole-brain cortex:
    # ------------------------------------------------------------------------
    # Extract FreeSurfer cerebrum cortical volume labels:
    FScortex = FSnoncortex.clone('Extract_FreeSurfer_cortex_labels')
    VolLabelFlow.add_nodes([FScortex])
    if do_input_fs_labels:
        mbFlow.connect(labelsNifti, 'labels', VolLabelFlow,
                       'Extract_FreeSurfer_cortex_labels.input_file')
    else:
        mbFlow.connect(labelsMGH2Nifti, 'output_file', VolLabelFlow,
                       'Extract_FreeSurfer_cortex_labels.input_file')
    FScortex.inputs.labels_to_keep = dkt.cerebrum_cortex_numbers
    FScortex.inputs.output_file = ''
    FScortex.inputs.second_file = ''

    # Propagate volume labels through whole-brain cortex:
    FSFillCortex = FSFillNoncortex.clone('Fill_cortex_with_FreeSurfer_labels')
    VolLabelFlow.add_nodes([FSFillCortex])
    if my_segments:
        FSFillCortex.inputs.mask = my_segments
    else:
        if use_ants:
            if overwrite_cerebrum_with_cerebellum:
                VolLabelFlow.connect(RemoveCerebellum, 'output_file',
                                     FSFillCortex, 'mask')
            else:
                VolLabelFlow.connect(JoinSegs, 'output_file',
                                     FSFillCortex, 'mask')
        else:
            VolLabelFlow.connect(FSsegments, 'output_file',
                                 FSFillCortex, 'mask')
    VolLabelFlow.connect(FScortex, 'output_file',
                         FSFillCortex, 'labels')
    FSFillCortex.inputs.mask_index = 2
    # ------------------------------------------------------------------------
    # Combine FreeSurfer label-filled whole-brain cortex and noncortex:
    # ------------------------------------------------------------------------
    CombineFSLabels = Node(name=
                           'Combine_FreeSurfer_cortex_noncortex_labels',
                           interface=Fn(function=overwrite_volume_labels,
                                        input_names=['source',
                                                     'target',
                                                     'output_file',
                                                     'ignore_labels',
                                                     'erase_labels'],
                                        output_names=['output_file']))
    VolLabelFlow.add_nodes([CombineFSLabels])
    VolLabelFlow.connect(FSFillNoncortex, 'output_file',
                         CombineFSLabels, 'source')
    #if modify_surface_labels:
    #    VolLabelFlow.connect(LRcortex, 'output_file',
    #                         CombineFSLabels, 'target')
    #else:
    VolLabelFlow.connect(FSFillCortex, 'output_file',
                         CombineFSLabels, 'target')
    CombineFSLabels.inputs.output_file = ''
    CombineFSLabels.inputs.ignore_labels = [0]
    CombineFSLabels.inputs.erase_labels = False
    if save_all and not overwrite_cerebrum_with_cerebellum:
        mbFlow.connect(VolLabelFlow,
               'Combine_FreeSurfer_cortex_noncortex_labels.output_file',
               Sink, 'labels.@freesurfer_filled')

    # ========================================================================
    # Fill whole-brain segmentation volumes with ANTs labels
    # ========================================================================
    if use_ants:
        # --------------------------------------------------------------------
        # Mask brain volume:
        # --------------------------------------------------------------------
        #MaskBrain = Node(name= 'Mask_brain',
        #                 interface=Fn(function=ImageMath,
        #                              input_names=['volume1',
        #                                           'volume2',
        #                                           'operator',
        #                                           'output_file'],
        #                              output_names=['output_file']))
        #VolLabelFlow.add_nodes([MaskBrain])
        #mbFlow.connect(MGH2Nifti, 'output_file',
        #               VolLabelFlow, 'Mask_brain.volume1')
        #mbFlow.connect(FetchANTs, 'mask', VolLabelFlow, 'Mask_brain.volume2')
        #MaskBrain.inputs.operator = 'm'
        #MaskBrain.inputs.output_file = ''
        # --------------------------------------------------------------------
        # Transform default atlas labels in MNI152 to subject via template:
        # --------------------------------------------------------------------
        xfm = Node(ApplyTransforms(), name='antsApplyTransforms')
        VolLabelFlow.add_nodes([xfm])
        xfm.inputs.dimension = 3
        xfm.inputs.default_value = 0
        xfm.inputs.interpolation = 'NearestNeighbor'
        xfm.inputs.invert_transform_flags = warp_inverse_Booleans
        xfm.inputs.output_image = 'ants_labels.nii.gz'
        if my_segments:
            xfm.inputs.reference_image = my_segments
        else:
            mbFlow.connect(FetchANTs, 'segments', VolLabelFlow,
                           'antsApplyTransforms.reference_image')
        if my_atlas:
            xfm.inputs.input_image = my_atlas
        else:
            mbFlow.connect(FetchAtlas, 'data_path',
                           VolLabelFlow, 'antsApplyTransforms.input_image')

        mbFlow.connect(WarpToSubjectFileList, 'string_list', VolLabelFlow,
                       'antsApplyTransforms.transforms')
        #if save_all:
        #    mbFlow.connect(VolLabelFlow, 'antsApplyTransforms.output_image',
        #                   Sink, 'labels.@antsRegistration')
        # --------------------------------------------------------------------
        # Extract ANTs cerebral cortical volume labels:
        # --------------------------------------------------------------------
        ANTsCortex = FSnoncortex.clone('Extract_ANTs_cortex_labels')
        VolLabelFlow.add_nodes([ANTsCortex])
        VolLabelFlow.connect(xfm, 'output_image', ANTsCortex, 'input_file')
        ANTsCortex.inputs.labels_to_keep = dkt.cerebrum_cortex_numbers
        ANTsCortex.inputs.output_file = ''
        ANTsCortex.inputs.second_file = ''
        # --------------------------------------------------------------------
        # Extract ANTs whole-brain noncortical volume labels:
        # --------------------------------------------------------------------
        ANTsNoncortex = ANTsCortex.clone('Extract_ANTs_noncortex_labels')
        VolLabelFlow.add_nodes([ANTsNoncortex])
        VolLabelFlow.connect(xfm, 'output_image', ANTsNoncortex, 'input_file')
        ANTsNoncortex.inputs.labels_to_keep = labels_to_fill
        ANTsNoncortex.inputs.output_file = ''
        # --------------------------------------------------------------------
        # Propagate ANTs whole-brain cortical volume labels through cortex:
        # --------------------------------------------------------------------
        ANTsFillCortex = FSFillNoncortex.clone('Fill_cortex_with_ANTs_labels')
        VolLabelFlow.add_nodes([ANTsFillCortex])
        if my_segments:
            ANTsFillCortex.inputs.mask = my_segments
        else:
            if overwrite_cerebrum_with_cerebellum:
                VolLabelFlow.connect(RemoveCerebellum, 'output_file',
                                     ANTsFillCortex, 'mask')
            else:
                VolLabelFlow.connect(JoinSegs, 'output_file',
                                     ANTsFillCortex, 'mask')
        VolLabelFlow.connect(ANTsCortex, 'output_file',
                             ANTsFillCortex, 'labels')
        ANTsFillCortex.inputs.mask_index = 2
        ANTsFillCortex.inputs.output_file = ''
        ANTsFillCortex.inputs.binarize = False
        ANTsFillCortex.inputs.stopvalue = ''
        # --------------------------------------------------------------------
        # Propagate ANTs whole-brain noncortical labels through noncortex:
        # --------------------------------------------------------------------
        if fill_noncortex_with_ants_labels:
            ANTsFillNoncortex = ANTsFillCortex.clone(
                'Fill_noncortex_with_ANTs_labels')
            VolLabelFlow.add_nodes([ANTsFillNoncortex])
            if my_segments:
                ANTsFillNoncortex.inputs.mask = my_segments
            else:
                if overwrite_cerebrum_with_cerebellum:
                    VolLabelFlow.connect(RemoveCerebellum, 'output_file',
                                         ANTsFillNoncortex, 'mask')
                else:
                    VolLabelFlow.connect(JoinSegs, 'output_file',
                                         ANTsFillNoncortex, 'mask')
            VolLabelFlow.connect(ANTsNoncortex, 'output_file',
                                 ANTsFillNoncortex, 'labels')
            ANTsFillNoncortex.inputs.mask_index = 3
        # --------------------------------------------------------------------
        # Combine ANTs label-filled cortex and label-filled noncortex:
        # --------------------------------------------------------------------
        CombineANTsLabels = CombineFSLabels.clone(
            'Combine_ANTs_cortex_noncortex_labels')
        VolLabelFlow.add_nodes([CombineANTsLabels])
        if fill_noncortex_with_ants_labels:
            VolLabelFlow.connect(ANTsFillNoncortex, 'output_file',
                                 CombineANTsLabels, 'source')
        else:
            VolLabelFlow.connect(ANTsNoncortex, 'output_file',
                                 CombineANTsLabels, 'source')
        VolLabelFlow.connect(ANTsFillCortex, 'output_file',
                             CombineANTsLabels, 'target')
        CombineANTsLabels.inputs.output_file = ''
        CombineANTsLabels.inputs.ignore_labels = [0]
        CombineANTsLabels.inputs.erase_labels = False
        if save_all and not overwrite_cerebrum_with_cerebellum:
            mbFlow.connect(VolLabelFlow, 
                           'Combine_ANTs_cortex_noncortex_labels.output_file',
                           Sink, 'labels.@ants_filled')

    # ========================================================================
    # Add FreeSurfer cerebellum labels
    # ========================================================================
    if overwrite_cerebrum_with_cerebellum:
        # --------------------------------------------------------------------
        # ...to FreeSurfer cerebrum labels:
        # --------------------------------------------------------------------
        AddFScerebellum = CombineFSLabels.clone(
            'FreeSurfer_cerebrum_cerebellum')
        VolLabelFlow.add_nodes([AddFScerebellum])
        VolLabelFlow.connect(FScerebellum, 'output_file',
                             AddFScerebellum, 'source')
        VolLabelFlow.connect(CombineFSLabels, 'output_file',
                             AddFScerebellum, 'target')
        AddFScerebellum.inputs.output_file = ''
        AddFScerebellum.inputs.ignore_labels = [0]
        AddFScerebellum.inputs.erase_labels = False
        if save_all:
            mbFlow.connect(VolLabelFlow,
                   'FreeSurfer_cerebrum_cerebellum.output_file',
                    Sink, 'labels.@freesurfer_filled_and_cerebellum')
        # --------------------------------------------------------------------
        # ...to ANTs cerebrum labels:
        # --------------------------------------------------------------------
        if use_ants:
            AddFScerebellum2ANTs = AddFScerebellum.clone(
                'FreeSurfer_cerebellum_ANTs_cerebrum')
            VolLabelFlow.add_nodes([AddFScerebellum2ANTs])
            VolLabelFlow.connect(FScerebellum, 'output_file',
                                 AddFScerebellum2ANTs, 'source')
            VolLabelFlow.connect(CombineANTsLabels, 'output_file',
                                 AddFScerebellum2ANTs, 'target')
            AddFScerebellum2ANTs.inputs.output_file = ''
            AddFScerebellum2ANTs.inputs.ignore_labels = [0]
            AddFScerebellum2ANTs.inputs.erase_labels = False
            if save_all:
                mbFlow.connect(VolLabelFlow,
                       'FreeSurfer_cerebellum_ANTs_cerebrum.output_file',
                       Sink, 'labels.@ants_filled_and_cerebellum')

    # ========================================================================
    #
    # Transform labels from added atlas(es) in MNI152 to subject
    #
    # ========================================================================
    if add_atlas_names and use_ants:
        # --------------------------------------------------------------------
        # Find atlas path that contains atlas name:
        # --------------------------------------------------------------------
        MatchAtlas = Node(name='Match_added_atlas',
                      interface=Fn(function=first_string_containing_substring,
                                   input_names=['substring',
                                                'List'],
                                   output_names=['first_matching_string']))
        VolLabelFlow.add_nodes([MatchAtlas])
        mbFlow.connect(InputAddAtlases, 'atlas', VolLabelFlow,
                       'Match_added_atlas.substring')
        MatchAtlas.inputs.List = atlases
        # --------------------------------------------------------------------
        # Transform atlas:
        # --------------------------------------------------------------------
        xfm2 = xfm.clone('Transform_added_atlases')
        VolLabelFlow.add_nodes([xfm2])
        xfm2.inputs.output_image = 'ants_added_atlas_labels.nii.gz'
        VolLabelFlow.connect(MatchAtlas, 'first_matching_string',
                             xfm2, 'input_image')
        if my_segments:
            xfm2.inputs.reference_image = my_segments
        else:
            mbFlow.connect(FetchANTs, 'segments', VolLabelFlow,
                           'Transform_added_atlases.reference_image')
        mbFlow.connect(WarpToSubjectFileList, 'string_list', VolLabelFlow,
                       'Transform_added_atlases.transforms')
        if save_all:
            mbFlow.connect(VolLabelFlow, 'Transform_added_atlases.output_image',
                           Sink, 'labels.@added_atlases')

    # ========================================================================
    #
    #   Volume feature shapes
    #
    # ========================================================================
    if do_shapes:

        VolShapeFlow = Workflow(name='Volume_feature_shapes')

        # ====================================================================
        # Measure volume of each region of a labeled image file
        # ====================================================================
        # --------------------------------------------------------------------
        # Volumes of the FreeSurfer filled labels:
        # --------------------------------------------------------------------
        FSlabelVolumes = Node(name='FreeSurfer_filled_label_volumes',
                              interface=Fn(function=volume_per_brain_region,
                                           input_names=['input_file',
                                                        'include_labels',
                                                        'exclude_labels',
                                                        'label_names',
                                                        'save_table',
                                                        'output_table'],
                                           output_names=['unique_labels',
                                                         'volumes',
                                                         'output_table']))
        VolShapeFlow.add_nodes([FSlabelVolumes])
        FSlabelVolumes.inputs.include_labels = dkt.label_numbers
        FSlabelVolumes.inputs.exclude_labels = []
        FSlabelVolumes.inputs.label_names = dkt.label_names
        FSlabelVolumes.inputs.save_table = True
        FSlabelVolumes.inputs.output_table = \
            'volume_for_each_freesurfer_label.csv'
        mbFlow.connect(VolLabelFlow,
               'Combine_FreeSurfer_cortex_noncortex_labels.output_file',
               VolShapeFlow, 'FreeSurfer_filled_label_volumes.input_file')
        mbFlow.connect(VolShapeFlow,
                       'FreeSurfer_filled_label_volumes.output_table',
                       Sink, 'tables.@volumes_of_freesurfer_labels')
        if use_ants:
            # ----------------------------------------------------------------
            # Volumes of the ANTs filled labels:
            # ----------------------------------------------------------------
            antsLabelVolumes = FSlabelVolumes.clone(
                'ANTs_filled_label_volumes')
            VolShapeFlow.add_nodes([antsLabelVolumes])
            antsLabelVolumes.inputs.exclude_labels = [-1,0]
            antsLabelVolumes.inputs.output_table = \
                'volume_for_each_ants_label.csv'
            if overwrite_cerebrum_with_cerebellum:
                mbFlow.connect(VolLabelFlow,
                       'FreeSurfer_cerebellum_ANTs_cerebrum.output_file',
                       VolShapeFlow, 'ANTs_filled_label_volumes.input_file')
            else:
                mbFlow.connect(VolLabelFlow,
                       'Combine_ANTs_cortex_noncortex_labels.output_file',
                       VolShapeFlow, 'ANTs_filled_label_volumes.input_file')
            mbFlow.connect(VolShapeFlow,
                           'ANTs_filled_label_volumes.output_table',
                           Sink, 'tables.@volumes_of_ants_labels')
            # ----------------------------------------------------------------
            # Volumes of labels in additional atlases transformed to subject:
            # ----------------------------------------------------------------
            if add_atlas_names:
                antsLabelVolumes2 = FSlabelVolumes.clone(
                    'Added_atlas_label_volumes')
                VolShapeFlow.add_nodes([antsLabelVolumes2])
                antsLabelVolumes2.inputs.label_names = []
                antsLabelVolumes2.inputs.include_labels = dkt.label_numbers
                antsLabelVolumes2.inputs.exclude_labels = [-1,0]
                antsLabelVolumes2.inputs.output_table = \
                    'volume_for_each_added_label.csv'
                mbFlow.connect(VolLabelFlow,
                               'Transform_added_atlases.output_image',
                               VolShapeFlow,
                               'Added_atlas_label_volumes.input_file')
                # Save table:
                mbFlow.connect(VolShapeFlow,
                               'Added_atlas_label_volumes.output_table',
                               Sink, 'tables.@volumes_of_added_atlas_labels')

        # ====================================================================
        # Measure volume, thickness of cortical regions of labeled image file
        # ====================================================================
        if do_thickinthehead:
            # ----------------------------------------------------------------
            # Thicknesses of the FreeSurfer cortical labels:
            # ----------------------------------------------------------------
            FSthicknesses = Node(
                name='FreeSurfer_filled_cortex_label_thicknesses',
                interface=Fn(function=thickinthehead,
                             input_names=['segmented_file',
                                          'labeled_file',
                                          'cortex_value',
                                          'noncortex_value',
                                          'labels',
                                          'names',
                                          'resize',
                                          'propagate',
                                          'output_dir',
                                          'save_table',
                                          'output_table'],
                             output_names=['label_volume_thickness',
                                           'output_table']))
            VolShapeFlow.add_nodes([FSthicknesses])
            if my_segments:
                FSthicknesses.inputs.segmented_file = my_segments
            else:
                if use_ants:
                    if overwrite_cerebrum_with_cerebellum:
                        mbFlow.connect(VolLabelFlow,
                            'Remove_cerebrum_cerebellum_overlap.output_file',
                            VolShapeFlow,
                            'FreeSurfer_filled_cortex_label_thicknesses.'
                            'segmented_file')
                    else:
                        mbFlow.connect(VolLabelFlow,
                            'Combine_FreeSurfer_ANTs_cerebrum_segments.'
                            'output_file',
                            VolShapeFlow,
                            'FreeSurfer_filled_cortex_label_thicknesses.'
                            'segmented_file')
                else:
                    mbFlow.connect(VolLabelFlow,
                        'FreeSurfer_cerebrum_labels_to_segments.output_file',
                        VolShapeFlow,
                        'FreeSurfer_filled_cortex_label_thicknesses.'
                        'segmented_file')
            mbFlow.connect(VolLabelFlow,
                'Combine_FreeSurfer_cortex_noncortex_labels.output_file',
                VolShapeFlow,
                'FreeSurfer_filled_cortex_label_thicknesses.labeled_file')
            FSthicknesses.inputs.cortex_value = 2
            FSthicknesses.inputs.noncortex_value = 3
            FSthicknesses.inputs.labels = dkt.cerebrum_cortex_numbers
            FSthicknesses.inputs.names = dkt.cerebrum_cortex_names
            FSthicknesses.inputs.resize = True
            FSthicknesses.inputs.propagate = False
            FSthicknesses.inputs.output_dir = ''
            FSthicknesses.inputs.save_table = True
            FSthicknesses.inputs.output_table = \
                'thickinthehead_per_freesurfer_cortex_label.csv'
            # Save table:
            mbFlow.connect(VolShapeFlow,
            'FreeSurfer_filled_cortex_label_thicknesses.output_table',
            Sink, 'tables.@thicknesses_of_freesurfer_labels')
            # ----------------------------------------------------------------
            # Thicknesses of the ANTs cortical labels:
            # ----------------------------------------------------------------
            if use_ants:
                ANTsThicknesses = FSthicknesses.\
                    clone('ANTs_filled_cortex_label_thicknesses')
                VolShapeFlow.add_nodes([ANTsThicknesses])
                if my_segments:
                    ANTsThicknesses.inputs.segmented_file = my_segments
                else:
                    if overwrite_cerebrum_with_cerebellum:
                        mbFlow.connect(VolLabelFlow,
                            'Remove_cerebrum_cerebellum_overlap.output_file',
                            VolShapeFlow,
                            'ANTs_filled_cortex_label_thicknesses.'
                            'segmented_file')
                    else:
                        mbFlow.connect(VolLabelFlow,
                            'Combine_FreeSurfer_ANTs_cerebrum_segments.'
                            'output_file',
                            VolShapeFlow,
                            'ANTs_filled_cortex_label_thicknesses.'
                            'segmented_file')
                if overwrite_cerebrum_with_cerebellum:
                    mbFlow.connect(VolLabelFlow,
                       'FreeSurfer_cerebellum_ANTs_cerebrum.output_file',
                       VolShapeFlow,
                       'ANTs_filled_cortex_label_thicknesses.labeled_file')
                else:
                    mbFlow.connect(VolLabelFlow,
                    'Combine_ANTs_cortex_noncortex_labels.output_file',
                    VolShapeFlow,
                    'ANTs_filled_cortex_label_thicknesses.labeled_file')
                ANTsThicknesses.inputs.labels = dkt.cerebrum_cortex_numbers
                ANTsThicknesses.inputs.names = dkt.cerebrum_cortex_names
                ANTsThicknesses.inputs.output_table = \
                    'thickinthehead_per_ants_cortex_label.csv'
                # Save table:
                mbFlow.connect(VolShapeFlow,
                   'ANTs_filled_cortex_label_thicknesses.output_table',
                   Sink, 'tables.@thicknesses_of_ants_filled_cortex_labels')


# ============================================================================
# ----------------------------------------------------------------------------
#
#   Run workflows
#
# ----------------------------------------------------------------------------
# ============================================================================
if __name__ == '__main__':

    from time import time
    time0 = time()

    # ------------------------------------------------------------------------
    # Workflow configuration: provenance tracking, content hashing, etc.:
    # ------------------------------------------------------------------------
    # config.enable_provenance()
    Flow.config['execution']['hash_method'] = 'content'
    # Flow.config['execution']['use_relative_paths'] = True

    # ------------------------------------------------------------------------
    # Generate a visual graph:
    # ------------------------------------------------------------------------
    graph_vis = args.graph
    if graph_vis:
        if graph_vis == 'exec':
            mbFlow.write_graph(graph2use=graph_vis, simple_form=False)
        else:
            if graph_vis == 'hier':
                graph_vis = 'hierarchical'
            mbFlow.write_graph(graph2use=graph_vis)

    # ------------------------------------------------------------------------
    # Debug: http://nipy.org/nipype/users/config_file.html#debug-configuration
    # ------------------------------------------------------------------------
    debug = False
    if debug:
        config.set('logging', 'workflow_level', 'DEBUG')
        logging.update_logging(config)
        mbFlow.config['execution']['stop_on_first_rerun'] = True
        nproc = 1

    # ------------------------------------------------------------------------
    # Run with or without a plugin:
    # ------------------------------------------------------------------------
    if args.plugin:
        if args.plugin_args:
            mbFlow.run(plugin=args.plugin, plugin_args=eval(args.plugin_args))
        else:
            mbFlow.run(plugin=args.plugin)
    elif nproc > 1:
        mbFlow.run(plugin='MultiProc',
                 plugin_args={'n_procs': nproc})
    else:
        mbFlow.run()  # Use all processors: Flow.run(plugin='MultiProc')

    print('Mindboggle run done! ({0:0.2f} seconds)'.format(time() - time0))
